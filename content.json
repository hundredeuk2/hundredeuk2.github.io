{"pages":[{"title":"About Me","text":"안녕하세요 빅데이터분석가가 되고싶지만 아직은 미숙한 학생입니다.","link":"/about/index.html"}],"posts":[{"title":"Time Series Data Validation","text":"시계열데이터 모델검증하기 (Timeseries Validation)공부를하다 시간에따라 발생되는 데이터를 다룰 때 k-fold validation을 쓰게되면 모순적인 부분이 있다고 생각이 들어 찾아본 결과 사이킷런의 TimeSeriesSplit을 사용하는 방법이 있었다. 기본적으로 모델검증을 위한 절차는 다음 그림을 참조하여 설명하자면 출처:scikit learn 훈련데이터를 모델검증을 위해 원하는 군집갯수로 나누어 또 다른 테스트 데이터와 훈련데이터로 나누어 N번 평가하는 것이다. 그래서 보통 N번의 MSE의 평균을 비교하여 가장 최적화된 모델이나 파라미터를 구하게 된다. 대표적으로 sklearn의 K-fold Validation이 있는데 이번에 공부하게 된건 TimeSeriesSplit이기 때문에 자세한 설명은 넘어가도록 한다. 여기서 의문이였던건 시계열데이터는 윗 그림과 같이 K-fold Validation을 쓰는 것이 옳지 않다는 것 같았다. split 1번부터 4번까지 테스트데이터를 기준으로 시간 측면에서 미래의 데이터를 훈련하여 현재의 데이터를 검증한다는 것이 나에게 있어서 논리오류를 범하는 것 같았다. 그래서 선택해본 것이 sklearn의 TimeseriesSplit이다. 출처:scikit learn K-fold와 조금 다르게 훈련데이터를 과거의 데이터를 기준으로 축적하여 쌓아나간다는 느낌으로 검증을 하게 된다. 다음과 같이 코딩을 하게 되는데 여기서 필자는 Simple DNN을 사용하여 이미 def함수를 사용하여 model을 구축했기 때문에 사용하고자 하는 model이나 방법론을 미리 구축해놓고 이름만 바꾸어 넣으면 될 듯 하다. 또한 여기서 필자는 fold의 갯수를 4개로 지정했다. 12345678910111213141516from sklearn.model_selection import KFold, TimeSeriesSplitn_fold = 4folds = TimeSeriesSplit(n_splits=n_fold)splits = folds.split(train)for fold_n, (train_index, valid_index) in enumerate(splits): print('Fold:', fold_n+1) X_train1, X_valid1 = x_train[x_col].iloc[train_index], x_train[x_col].iloc[valid_index] y_train1, y_valid1 = y_train.iloc[train_index], y_train.iloc[valid_index] model = build_model() history = model.fit(X_train1, y_train1, validation_data=(X_valid1, y_valid1), epochs=num_epochs, batch_size=1, verbose=0) mae_history = history.history['val_mae'] all_mae_histories.append(mae_history) 이런식으로 코딩을 하게 된 후에 all_mae_histories에 MAE스코어가 담겨있기 때문에 평균 MAE를 보고 파라미터 조정 또는 성능좋은 모델을 사용하면 된다. np.mean(all_mae_histories) 필자는 모델이 300회부터 오버피팅(overfitting)이 된 걸 확인 할 수 있었다.","link":"/2021/01/17/21-01-17%20first/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/01/14/hello-world/"},{"title":"블로그 개설","text":"나만의 블로그 개설!!21년 새로운 목표 그리고 나의 자기개발과 성실성을 증가시키기 위함을 목적으로 만든 블로그이다. 아직 매우 미숙하고 블로그 자체도 제데로 만들어져있지 않지만 이렇게 게시글을 올리는 행동으로 계속 블로그를 만들어야되는 이유에 대해 상기시키고 빠른 시일내에 블로그 프레임을 전부 완성시키도록 하겠다. 뭐 하나 제데로 하는 것이 없는 나에게 이 시도는 매우 큰 의미를 가지고 있고, 이 블로그를 계기로 나의 지난 과거를 되돌아봄과 앞으로의 열정을 불태워보겠다. 주로 코딩, 프로젝트내용 그리고 내 생활면에선 일기나 내 생각을 반영한 에세이, 취미생활에 대한 진행도나 내역을 적을 계획이다.","link":"/2021/01/14/%EC%B2%AB%EA%B8%80/"},{"title":"나비효과","text":"나비효과하나의 작은 움직임 평소의 사소한 행동이 긍정적이든 부정적이든 어떤 한 사태에 대한 거대한 파급효과를 낳게 된다는 것을 알게 되면 그 행동을 하시겠습니까? 나비의 작은 날갯짓 한 번이 지구 반대편의 폭풍을 만든다 라는 의미의 나비효과는 영화의 제목으로도 잘 알려져 있다. 즉, 사소한 사건 하나가 나중에 커다란 효과를 가져올 수 있다는 의미라고 할 수 있겠다. 예를 들어, 유튜브의 한 동영상 중 거북이 콧구멍에 박혀있는 플라스틱 조각을 제거해 주는 동영상이 있다. 우연찮게 그 동영상을 보았고 그 나무젓가락이 만약 어릴 적 친구들과 바닷가에서 컵라면을 먹고 귀찮아 아무렇지 않게 던진 그 나무젓가락이었다면 그 귀찮은 행동 한 번을 참으시겠습니까? 필자는 유튜브를 보던 중 유명한 심리학자 조던 피터슨이 나와 강연에서 한 말이 뼈아팠지만 생각해보니 인정하고 이 글을 써야겠다는 생각을 들게 하였다. “자기의 방조차 깨끗이 치우고 유지도 못하는데 사회적 문제를 운운하면서 문제를 해결하겠다는 마음은 어불성설이다” 부끄럽지만 이 동영상을 보고 필자는 방을 치우기 시작했는데 도중에 감당이 안돼서 멈췄다. 생각보다 매우 어려웠던 이유가 물건들의 알수없는 위치와 정리정돈 자체가 습관화되어있지 않았고 체계적으로 실행하지 못했기 때문에 치우다가도 더욱 어질러지는 현상을 발견하였다. 물론 모두가 필자와 같진 않다고 믿는다. 다만, 피식하고 웃을 정도의 사소한 습관 또는 행동 하나도 못하는데 전 세계적인 문제 아니, 지금 당장 취업을 못하는 이유를 밖에서 찾고 있는 나 자신이 보였다. 이렇게 사소하다고 생각한 행동 하나조차도 못하는 필자는 게으름과 나태함을 인정하고 더더욱 사소한 행동부터 고쳐서 점차 나아지는 행동을 보이기로 했다. 그 행동의 첫 번째 결실은 아침에 일어나 이불 정리와 외출 후 옷가지 정리이다. 방청소 조차도 못하는데 더더욱 더럽힐 순 없기 때문에 이러한 습관을 들이 고나서야 천천히 내방의 모든 것들을 정리정돈을 하고 점점 더 높은 상위의 습관을 계획하고 갖춰나갈수 있었다. 실행하고 3-4개월쯤 되었을 때, 어느덧 이불 정리가 안되어있으면 불편하고 깨끗하지 않다는 생각을 할 정도로 바뀌었고 아침에 늦게 일어나고 밍기적거리는 게으른 습관들이 이불 정리를 통해 내가 기껏 깨끗이 정리해 둔 자리를 다시 누워 더럽히는 상상을 하니 점차 싫어져서 이른 기상을 하게 되는 연쇄효과도 생겼다. 일찍 일어나는 것이 무조건 좋다는 건 아니지만 필자가 가지고 싶어 했던 습관 중 하나이기 때문에 긍정적으로 보고 현재는 좀 더 많은 계획과 행동을 실행하는 중이다. 기타를 쳐보고 싶었다? 그럼 우선 기타를 사보자 아니, 최소한 유튜브에서 다른 것보다 기타 동영상을 한번 봐보자. 자신이 다이어트 또는 운동을 하고 싶다? 그럼 일단 누워있을 테니 일어나 보자 일어나서 앞에 산책 단 5분이라도 다녀오자. 내일 해야지 다음에 해야지 이 구간을 눈감고 딱 한 번만 참고 나비의 날갯짓을 하게 된다면 그 앞에 벌어질 현상은 바람이 되어 거대한 폭풍이 되어 자신의 삶에 어떤 영향력을 끼칠지 모르니까.","link":"/2021/02/02/%EB%82%98%EB%B9%84%ED%9A%A8%EA%B3%BC/"},{"title":"21&#x2F;03&#x2F;01 끄적끄적","text":"착각“익숙함에 속아 소중함을 잃지 말자” 착각에 빠져 살고있다는 것을 알게 되었을 때는 내가 믿고 있었던 부분이나 확실하다고 생각이 든 부분에서 문제점을 발견했을 때라고 생각이 든다. 옛말인지 현대어인지 잘 모르겠으나 누구나 한번쯤은 들어본 말이 있다. “익숙함에 속아 소중함을 잃지 말자” 되게 비교적 모순적인 말이라고 생각이 든 것은 필자가 정말 철이 없다고 느낀 시절에 항상 잃고나서야 그것이 소중한 것임을 깨달아서 잃고나서도 매번 잘해주지 못했다는 죄책감에 사로잡혀 매일을 힘들게 보낸적이 많다. 물론 지금도 때때로 그런 감정을 느끼지만 예전엔 잃기전에 잘해야된다는 것을 알면서도 점차 소홀해 하는 내 자신의 모습을 볼 수 있었다. 그리고 이 글을 작성하게끔 만든 내 자신은 예전 대학원에 붙어 마지막 기회라고 생각하고 놀더라도 하고자하던 바를 끝마치고 놀겠다고 다짐했던 그 의지가 어느샌가 잘하고 있다는 착각속에 빠져 나를 나태하게 만들었다. 이 기회는 내 많은 것을 포기하고 얻어낸 기회임을 알기때문에 더더욱 내 자신에게 화가 난다. 그동안 포기했던 것들이 이유없는 댓가로 인해 포기를 한 것 같기 때문이다. 즉, 내 자신에게 남는게 없다 라고 생각이 들어서 더욱 화가 났다는 것이다. 그리고 사실 그것을 알았을 때는 한두달 전이지만 매번 나에게만은 관대하게 생각했기때문에 심각성을 몰랐다. 그렇기 때문에 조금 나를 표현하는 조금 공개적인 장소에 이 글을 적으면서 내 의지를 다시 불태워야겠다는 생각을 했다. 그리고 글 솜씨가 좋지 않은걸 알기 때문에 이렇게 부족한 나라도 꾸준한 기록과 함께 성장하는 모습을 스스로 보게된다면 익숙함에 속지 않고 나 혼자 스스로 익숙함에 매번 녹아들지 않을까 싶다. 항상 잃고나서야 소중함을 알게되는 필자야, 그동안 익숙함에 속아 시간을 잃었으니 앞으로의 내 청춘을 위해 더이상 속지말고 앞으로 나아가자!","link":"/2021/03/01/21-03-01-%EB%81%84%EC%A0%81%EB%81%84%EC%A0%81/"},{"title":"전처리 (1)","text":"데이터 전처리개인적으로 아파트 실거래가와 매매거래수에 관한 분석을 하는 도중 서울특별시를 분석하기에 앞서 모든 행정구를 따져봐야하는 생각이 들어 귀찮게 일일이 손수 코딩을 해도 되지만 반복문을 사용하면 좀 더 쉬운 길을 택할 수 있을 것 같았고, 이런 부분들을 하나둘씩 해결해가면 좀 더 실력이 늘 것 같아 많은 검색과 공부를 통해 구현을 해보았다. 다음은 내가 막힌 몇 부분을 해결했지만 다음에도 적용을 하기 위함과 이렇게 누군가에게 가르칠정도의 정리를 하게 되면 나 또한 잊어먹지 않을 것 같아 적어보았다. 반복(for)문을 사용하여 각기 다른 변수명을 가진 데이터프레임 생성 우선 서울 모든 행정구를 하나의 list에 담아보았다. 1gu_list = ['강남구', '강동구', '강북구', '강서구', '관악구', '광진구', '구로구', '금천구', '노원구', '도봉구', '동대문구', '동작구', '마포구', '서대문구', '서초구', '성동구', '성북구', '송파구', '양천구', '영등포구', '용산구', '은평구', '종로구', '중구', '중랑구'] 이후 미리 월별 실거래가 평균을 낸 데이터프레임에서 “시군구”안에 특정 구를 포함한 데이터를 df_mean[행정구] 형태의 25개 데이터 프레임을 만들려면 다음과 같은 코드를 사용하면 된다. 123for i in gu_list: a = price_mean_df.loc[price_mean_df['시군구'].str.contains(i)] globals()['df_mean_{}'.format(i)] = a.groupby(['계약년월'])['거래금액(만원)'].mean().reset_index()","link":"/2021/03/16/%EC%A0%84%EC%B2%98%EB%A6%AC-1/"}],"tags":[],"categories":[{"name":"M&#x2F;L","slug":"M-L","link":"/categories/M-L/"},{"name":"-끄적끄적","slug":"끄적끄적","link":"/categories/%EB%81%84%EC%A0%81%EB%81%84%EC%A0%81/"},{"name":"Validation","slug":"M-L/Validation","link":"/categories/M-L/Validation/"},{"name":"-전처리","slug":"전처리","link":"/categories/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]}